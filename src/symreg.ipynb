{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Symbolic Regression Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from icecream import ic\n",
    "from gxgp.node import Node as GXNode\n",
    "from gxgp.draw import draw\n",
    "\n",
    "data = np.load('../data/problem_1.npz')  \n",
    "x_validation = data['x'] \n",
    "y_validation = data['y']\n",
    "\n",
    "num_vars = x_validation.shape[0]\n",
    "VARIABLES = [f\"x{i}\" for i in range(num_vars)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tree Node Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import operator\n",
    "import random\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, value, children=None):\n",
    "        \"\"\"\n",
    "        value could be:\n",
    "            - a float (constant);\n",
    "            - a string (a variable name like 'x0','x1', ...):\n",
    "            - a Python function ( math.sin, operator.add, ...)\n",
    "        \"\"\"\n",
    "        self.value = value\n",
    "        self.children = children if children is not None else []\n",
    "    \n",
    "    def is_leaf(self):\n",
    "        return len(self.children) == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tree Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(node: Node, x: np.ndarray) -> np.ndarray:\n",
    "    if node.is_leaf():\n",
    "        val = node.value\n",
    "        # Case 1: constant\n",
    "        if isinstance(val, (float, int)) and not isinstance(val, bool):\n",
    "            return np.full(x.shape[1], float(val))\n",
    "        \n",
    "        # Case 2: variable\n",
    "        if isinstance(val, str) and val.startswith(\"x\"):\n",
    "            try:\n",
    "                idx = VARIABLES.index(val)\n",
    "            except ValueError:\n",
    "                raise ValueError(f\"Unknown Variable: {val}\")\n",
    "            return x[idx, :]  \n",
    "    \n",
    "        # Fallback\n",
    "        raise ValueError(f\"Unknown leaf value: {val}\")\n",
    "\n",
    "    else:\n",
    "        # Case 3: internal node\n",
    "        op = node.value\n",
    "        children_values = [evaluate(child, x) for child in node.children]\n",
    "\n",
    "        if len(children_values) == 1:\n",
    "            return safe_apply_unary(op, children_values[0])\n",
    "        elif len(children_values) == 2:\n",
    "            return safe_apply_binary(op, children_values[0], children_values[1])\n",
    "        else:\n",
    "            raise ValueError(\"Children number not supported\")\n",
    "\n",
    "\n",
    "        \n",
    "def protected_div(a, b):\n",
    "    if abs(b) < 1e-12:\n",
    "        return 1.0\n",
    "    return a / b\n",
    "\n",
    "def protected_sqrt(x):\n",
    "    if x < 0:\n",
    "        return math.sqrt(abs(x))\n",
    "    return math.sqrt(x)\n",
    "\n",
    "def protected_log10(x):\n",
    "    if x <= 0:\n",
    "        return 0.0\n",
    "    return math.log10(x)\n",
    "\n",
    "def safe_apply_unary(func, arr):\n",
    "    vfunc = np.frompyfunc(func, 1, 1)\n",
    "    return vfunc(arr).astype(float)\n",
    "\n",
    "def protected_exp(x):\n",
    "    # Overflow condition\n",
    "    if x > 700:\n",
    "        return math.exp(700)  \n",
    "    # Underflow condition\n",
    "    elif x < -700:\n",
    "        return math.exp(-700)\n",
    "    else:\n",
    "        return math.exp(x)\n",
    "def safe_apply_binary(func, arr1, arr2):\n",
    "    vfunc = np.frompyfunc(func, 2, 1)\n",
    "    return vfunc(arr1, arr2).astype(float)\n",
    "\n",
    "\n",
    "DEPTH_MAX = 6\n",
    "CONST_MAX = 10\n",
    "CONST_MIN = -10\n",
    "UNARY_FUNCS = [math.sin, math.cos, operator.neg, abs, protected_log10, protected_sqrt, protected_exp]\n",
    "BINARY_FUNCS = [operator.add, operator.sub, operator.mul, protected_div]\n",
    "\n",
    "def generate_subtree_forced_var(\n",
    "    var: str,  \n",
    "    depth: int,\n",
    "    constant_min=CONST_MIN,\n",
    "    constant_max=CONST_MAX\n",
    ") -> Node:\n",
    "    \"\"\"\n",
    "    Generate recursively a subtree with a variable var at the root.\n",
    "    \"\"\"\n",
    "    if depth <= 0:\n",
    "        return Node(var)\n",
    "    else:\n",
    "        if random.random() < 0.3:\n",
    "            op = random.choice(UNARY_FUNCS)\n",
    "            child = generate_subtree_forced_var(var, depth - 1, constant_min, constant_max)\n",
    "            return Node(op, [child])\n",
    "        else:\n",
    "            op = random.choice(BINARY_FUNCS)\n",
    "            left = generate_subtree_forced_var(var, depth - 1, constant_min, constant_max)\n",
    "            right = generate_random_subtree(depth - 1, constant_min=constant_min, constant_max=constant_max)\n",
    "            return Node(op, [left, right])\n",
    "\n",
    "\n",
    "        \n",
    "def generate_random_subtree(depth, constant_min=CONST_MIN, constant_max=CONST_MAX) -> Node:\n",
    "    \"\"\"\n",
    "    Generate a random subtree without any constraint on the variables.\n",
    "    \"\"\"\n",
    "    variables = VARIABLES.copy()\n",
    "\n",
    "    if depth <= 0 or (depth > 1 and random.random() < 0.3):\n",
    "        if random.random() < 0.5 and variables:\n",
    "            # Choosing a random variable from the subset\n",
    "            var = random.choice(variables)\n",
    "            return Node(var)\n",
    "        else:\n",
    "            #Choosing a random constant\n",
    "            return Node(random.uniform(constant_min, constant_max))\n",
    "    else:\n",
    "        if random.random() < 0.3 and UNARY_FUNCS:\n",
    "            op = random.choice(UNARY_FUNCS)\n",
    "            child = generate_random_subtree(depth - 1, constant_min=constant_min, constant_max=constant_max)\n",
    "            return Node(op, [child])\n",
    "        else:\n",
    "            op = random.choice(BINARY_FUNCS)\n",
    "            left = generate_random_subtree(depth - 1, constant_min=constant_min, constant_max=constant_max)\n",
    "            right = generate_random_subtree(depth - 1, constant_min=constant_min, constant_max=constant_max)\n",
    "            return Node(op, [left, right])\n",
    "\n",
    "def generate_random_tree(depth=DEPTH_MAX, constant_min=CONST_MIN, constant_max=CONST_MAX) -> Node:\n",
    "    \"\"\"\n",
    "    Generate a tree which includes all the variables.\n",
    "    \"\"\"\n",
    "    if len(VARIABLES) == 0:\n",
    "        raise ValueError(\"There is no variable available.\")\n",
    "    elif len(VARIABLES) == 1:\n",
    "        var = VARIABLES[0]\n",
    "        return generate_subtree_forced_var(var, depth, constant_min=constant_min, constant_max=constant_max)\n",
    "    else:\n",
    "        if random.random() < 0.3 and UNARY_FUNCS:\n",
    "            # Unary operator\n",
    "            op = random.choice(UNARY_FUNCS)\n",
    "            child = generate_random_subtree(depth - 1, constant_min=constant_min, constant_max=constant_max)\n",
    "            return Node(op, [child])\n",
    "        elif BINARY_FUNCS:\n",
    "            # Binary operator\n",
    "            op = random.choice(BINARY_FUNCS)\n",
    "            left_subtree = generate_random_subtree(depth - 1, constant_min=constant_min, constant_max=constant_max)\n",
    "            right_subtree = generate_random_subtree(depth - 1, constant_min=constant_min, constant_max=constant_max)\n",
    "            return Node(op, [left_subtree, right_subtree])\n",
    "        else:\n",
    "            # Fallback: generate a random subtree without constraints\n",
    "            return generate_random_subtree(depth, constant_min=constant_min, constant_max=constant_max)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expression String"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DISPLAY_NAME_MAP = {operator.add: '+', operator.sub: '-', operator.mul: '*', protected_div: '/', math.sin: 'sin', math.cos: 'cos',\n",
    "    operator.neg: '-', abs: 'abs', protected_log10: 'log10', protected_sqrt: 'sqrt', protected_exp: 'exp',\n",
    "}\n",
    "\n",
    "def tree_to_string(node: Node) -> str:\n",
    "    \"\"\" It Returns a string representation of the tree node \"\"\"\n",
    "    # Case 1: Leaf node\n",
    "    if node.is_leaf():\n",
    "        val = node.value\n",
    "        # Constant\n",
    "        if isinstance(val, (float, int)) and not isinstance(val, bool):\n",
    "            return f\"{float(val):.3f}\"\n",
    "        # Variable\n",
    "        if isinstance(val, str) and val.startswith(\"x\"):\n",
    "            idx = val[1]  \n",
    "            return f\"x{idx}\"\n",
    "        # fallback\n",
    "        return str(val)\n",
    "\n",
    "    # Caso 2: Internal node (Operator)\n",
    "    else:\n",
    "        op = node.value\n",
    "        child_strs = [tree_to_string(child) for child in node.children]\n",
    "        op_name = DISPLAY_NAME_MAP.get(op, op.__name__ if hasattr(op, '__name__') else str(op))\n",
    "        # Unary operator\n",
    "        if len(child_strs) == 1:\n",
    "            return f\"{op_name}({child_strs[0]})\"\n",
    "\n",
    "        # Binary operator\n",
    "        elif len(child_strs) == 2:\n",
    "            left_str, right_str = child_strs\n",
    "            return f\"({left_str} {op_name} {right_str})\"\n",
    "        else:\n",
    "            raise ValueError(\"Children number not supported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversion Function\n",
    "They are useful to convert the tree generated my algorithm into the format the draw function in gxgp directory needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_var_func(var_name):\n",
    "    \"\"\"It creates a function that returns the value of a variable from the keyword arguments.\"\"\"\n",
    "    def var_func(**kwargs):\n",
    "        return kwargs[var_name]\n",
    "    var_func.__name__ = var_name\n",
    "    return var_func\n",
    "\n",
    "def make_const_func(const_val):\n",
    "    \"\"\"It creates a function that returns a constant value.\"\"\"\n",
    "    def const_func(**kwargs):\n",
    "        return const_val\n",
    "    const_func.__name__ = f\"{const_val:.3f}\"\n",
    "    return const_func\n",
    "\n",
    "def convert_to_gxgp_node(my_node, subtree=None) -> GXNode:\n",
    "    \"\"\"\n",
    "    It converts a Node object to a GXNode object.\n",
    "    \"\"\"\n",
    "    if subtree is None:\n",
    "        subtree = set()\n",
    "\n",
    "    if my_node.is_leaf():\n",
    "        val = my_node.value\n",
    "        if isinstance(val, str):\n",
    "            \n",
    "            var_name = val\n",
    "            var_func = make_var_func(var_name)\n",
    "            display_name = DISPLAY_NAME_MAP.get(var_name, var_name)\n",
    "            gx_node = GXNode(var_func, [], name=display_name)      \n",
    "        else:\n",
    "            const_val = float(val)\n",
    "            const_func = make_const_func(const_val)\n",
    "            display_name = f\"{const_val:.3f}\"  \n",
    "            gx_node = GXNode(const_func, [], name=display_name) \n",
    "        subtree.add(gx_node)\n",
    "        return gx_node\n",
    "    else:\n",
    "        op = my_node.value\n",
    "        gx_op = op  \n",
    "        display_name = DISPLAY_NAME_MAP.get(op, op.__name__ if hasattr(op, '__name__') else str(op))\n",
    "        converted_children = []\n",
    "        for child in my_node.children if my_node.children is not None else []:\n",
    "            converted_child = convert_to_gxgp_node(child, subtree)\n",
    "            converted_children.append(converted_child)\n",
    "\n",
    "        gx_node = GXNode(gx_op, converted_children, name=display_name)\n",
    "        subtree.add(gx_node)\n",
    "        return gx_node\n",
    "    \n",
    "\n",
    "\n",
    "import math\n",
    "import operator\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitness Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fitness(individual: Node, x: np.ndarray, y: np.ndarray) -> float:\n",
    "    \"\"\" It calculates the fitness of an individual. \"\"\"\n",
    "    try:\n",
    "        y_pred = evaluate(individual, x)\n",
    "        if not np.isfinite(y_pred).all():\n",
    "            return 1e10 \n",
    "        mse = np.mean((y - y_pred)**2)\n",
    "        return mse\n",
    "    except (ValueError, ZeroDivisionError):\n",
    "        return 1e10 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tournament_selection(population, x, y, k=3):\n",
    "    \"\"\" It returns the best individual from a random tournament. \"\"\"\n",
    "    contenders = random.sample(population, k)\n",
    "    best = min(contenders, key=lambda ind: fitness(ind, x, y))\n",
    "    return best\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crossover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossover(parent1: Node, parent2: Node) -> Node:\n",
    "    \"\"\"Crossover between two parents.\"\"\"\n",
    "    child1 = clone_tree(parent1)\n",
    "    child2 = clone_tree(parent2)\n",
    "    \n",
    "    internal_nodes1 = [node for node in get_all_nodes(child1) if not node.is_leaf()]\n",
    "    internal_nodes2 = [node for node in get_all_nodes(child2) if not node.is_leaf()]\n",
    "    \n",
    "    # If there are internal nodes in both trees it selects a random internal node from each tree and swaps the subtrees\n",
    "    if internal_nodes1 and internal_nodes2:\n",
    "        node1 = random.choice(internal_nodes1)\n",
    "        node2 = random.choice(internal_nodes2)\n",
    "    \n",
    "        node1.value, node1.children, node2.value, node2.children = node2.value, node2.children, node1.value, node1.children\n",
    "    \n",
    "    return child1\n",
    "\n",
    "\n",
    "def get_random_node(tree: Node) -> Node:\n",
    "    \"\"\" It returns a random node from the tree. \"\"\"\n",
    "    all_nodes = get_all_nodes(tree)\n",
    "    return random.choice(all_nodes)\n",
    "\n",
    "def get_all_nodes(tree: Node) -> list:\n",
    "    \"\"\" It returns a list with all the nodes of the tree. \"\"\"\n",
    "    nodes = [tree]\n",
    "    for c in tree.children:\n",
    "        nodes += get_all_nodes(c)\n",
    "    return nodes\n",
    "\n",
    "def clone_tree(node: Node) -> Node:\n",
    "    \"\"\" It creates a deep copy of the tree. \"\"\"\n",
    "    new_node = Node(node.value)\n",
    "    new_node.children = [clone_tree(c) for c in node.children]\n",
    "    return new_node\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mutation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mutate(individual: Node, mutation_rate=0.4) -> Node:\n",
    "    \"\"\"With a certain probability it mutates a node of the tree.\"\"\"\n",
    "    mutant = clone_tree(individual)\n",
    "    if random.random() < mutation_rate:\n",
    "        internal_nodes = [node for node in get_all_nodes(mutant) if not node.is_leaf()]\n",
    "        leaf_nodes = [node for node in get_all_nodes(mutant) if node.is_leaf()]\n",
    "        \n",
    "        # With a certain probability it mutates an internal node, otherwise it mutates a leaf node.\n",
    "        if internal_nodes and (not leaf_nodes or random.random() < 0.5):\n",
    "            \n",
    "            node_to_mutate = random.choice(internal_nodes)\n",
    "            new_subtree = generate_random_tree(depth=DEPTH_MAX, constant_min=CONST_MIN, constant_max=CONST_MAX)\n",
    "            node_to_mutate.value = new_subtree.value\n",
    "            node_to_mutate.children = new_subtree.children\n",
    "        elif leaf_nodes:\n",
    "            node_to_mutate = random.choice(leaf_nodes)\n",
    "            if random.random() < 0.5:\n",
    "                new_var = random.choice(VARIABLES)\n",
    "                node_to_mutate.value = new_var\n",
    "            else:\n",
    "                node_to_mutate.value = random.uniform(CONST_MIN, CONST_MAX)\n",
    "                node_to_mutate.children = []\n",
    "    \n",
    "    return mutant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cutting the Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enforce_max_depth(node, max_depth=3, current_depth=0):\n",
    "    \"\"\"It reduces the depth of the tree to a maximum depth.\"\"\"\n",
    "    if current_depth >= max_depth:\n",
    "        if VARIABLES:\n",
    "            node.value = random.choice(VARIABLES + [random.uniform(CONST_MIN, CONST_MAX)])\n",
    "        else:\n",
    "            node.value = random.uniform(CONST_MIN, CONST_MAX)\n",
    "        node.children = []\n",
    "    else:\n",
    "        for c in node.children:\n",
    "            enforce_max_depth(c, max_depth, current_depth+1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Genetic Programming Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_genetic_programming(x: np.ndarray, y: np.ndarray,\n",
    "                            population_size=100,\n",
    "                            generations=50,\n",
    "                            elite_size=2,\n",
    "                            max_depth=6):\n",
    "    \"\"\" It executes the genetic programming algorithm. \"\"\"\n",
    "\n",
    "    # 1) Ramped Half-and-Half Initialization\n",
    "    population = []\n",
    "    half_pop = population_size // 2\n",
    "    for i in range(half_pop):\n",
    "        tree = generate_random_tree(depth=2)\n",
    "        enforce_max_depth(tree, max_depth=max_depth)\n",
    "        population.append(tree)\n",
    "\n",
    "    for i in range(population_size - half_pop):\n",
    "        tree = generate_random_tree(depth=4)\n",
    "        enforce_max_depth(tree, max_depth=max_depth)\n",
    "        population.append(tree)\n",
    "    \n",
    "    best_overall = None\n",
    "    best_fitness = float('inf')\n",
    "    \n",
    "    # Hall of Fame with the best individual of each generation\n",
    "    hall_of_fame = []  \n",
    "\n",
    "    for g in range(generations):\n",
    "        # 2) Fitness Evaluation\n",
    "        scored_pop = [(ind, fitness(ind, x, y)) for ind in population]\n",
    "        scored_pop.sort(key=lambda x: x[1]) \n",
    "        \n",
    "        best_current, best_current_fit = scored_pop[0]\n",
    "      \n",
    "        if best_current_fit < best_fitness:\n",
    "            best_overall = clone_tree(best_current)\n",
    "            best_fitness = best_current_fit\n",
    "        \n",
    "        best_str = tree_to_string(best_current)\n",
    "        print(f\"[Gen {g}] Best MSE: {best_current_fit:.5f} => {best_str}\")\n",
    "        \n",
    "        hall_of_fame.append((clone_tree(best_current), best_current_fit))\n",
    "        \n",
    "        new_population = [ind for ind, fit in scored_pop[:elite_size]]\n",
    "        \n",
    "        # 4) Reproduction\n",
    "        while len(new_population) < population_size:\n",
    "            # Selection\n",
    "            p1 = tournament_selection(population, x, y, k=3)\n",
    "            p2 = tournament_selection(population, x, y, k=3)\n",
    "            # Crossover\n",
    "            offspring = crossover(p1, p2)\n",
    "            # Mutation\n",
    "            offspring = mutate(offspring)\n",
    "            # Enforce max depth to reduce the size of the tree\n",
    "            enforce_max_depth(offspring, max_depth=max_depth)\n",
    "            new_population.append(offspring)\n",
    "        \n",
    "        population = new_population\n",
    "    \n",
    "    return best_overall, best_fitness, hall_of_fame\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Variables: {VARIABLES}\")\n",
    "print(f\"x_validation shape: {x_validation.shape}\")\n",
    "print(f\"y_validation shape: {y_validation.shape}\")\n",
    "\n",
    "N = x_validation.shape[1]\n",
    "print(f\"Total number of samples = {N}\")\n",
    "\n",
    "\"\"\"\n",
    "print(\"\\nTraining:\\n\")\n",
    "TRAIN_SIZE = N // 10\n",
    "print(f\"TRAIN_SIZE = {TRAIN_SIZE}\")\n",
    "\n",
    "train_indexes = np.random.choice(N, size=TRAIN_SIZE, replace=False)\n",
    "\n",
    "x_train = x_validation[:, train_indexes]\n",
    "y_train = y_validation[train_indexes]\n",
    "\n",
    "best_individual_training, best_fit_training, hall_of_fame_training = run_genetic_programming(\n",
    "    x_train, y_train,\n",
    "    population_size=10_000,\n",
    "    generations=100,\n",
    "    elite_size=2\n",
    ")\n",
    "\n",
    "\n",
    "expr_str_training = tree_to_string(best_individual_training)\n",
    "print(f\"\\nBest expression found = {expr_str_training}, MSE = {best_fit_training}\")\n",
    "\n",
    "gx_best_individual_training = convert_to_gxgp_node(best_individual_training)\n",
    "print(\"Final Expression Tree (GP on training set):\")\n",
    "draw(gx_best_individual_training)\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Test:\\n\")\n",
    "best_individual, best_fit, hall_of_fame = run_genetic_programming(\n",
    "    x_validation, y_validation,\n",
    "    population_size=10_000,\n",
    "    generations=100,\n",
    "    elite_size=2\n",
    ")\n",
    "\n",
    "expr_str = tree_to_string(best_individual)\n",
    "print(f\"\\nBest expression found = {expr_str}, MSE = {best_fit}\")\n",
    "\n",
    "gx_best_individual = convert_to_gxgp_node(best_individual)\n",
    "print(\"Final Expression Tree (GP on test set):\")\n",
    "draw(gx_best_individual)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CI2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
